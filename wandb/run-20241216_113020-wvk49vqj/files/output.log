  0%|                                                                                                                                                                                          | 0/489 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/jovyan/.local/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
Could not estimate the number of tokens of the input, floating-point operations will not be computed
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 489/489 [20:19<00:00,  2.49s/it]
{'loss': 0.691, 'grad_norm': 0.25362429022789, 'learning_rate': 9.386503067484662e-07, 'epoch': 0.06}
{'loss': 0.681, 'grad_norm': 0.3754478693008423, 'learning_rate': 8.773006134969325e-07, 'epoch': 0.12}
{'loss': 0.6636, 'grad_norm': 0.6520679593086243, 'learning_rate': 8.159509202453987e-07, 'epoch': 0.18}
{'loss': 0.625, 'grad_norm': 1.1436747312545776, 'learning_rate': 7.54601226993865e-07, 'epoch': 0.25}
{'loss': 0.5524, 'grad_norm': 1.617213249206543, 'learning_rate': 6.932515337423313e-07, 'epoch': 0.31}
{'loss': 0.4632, 'grad_norm': 1.4771738052368164, 'learning_rate': 6.319018404907975e-07, 'epoch': 0.37}
{'loss': 0.3843, 'grad_norm': 1.3768483400344849, 'learning_rate': 5.705521472392638e-07, 'epoch': 0.43}
{'loss': 0.324, 'grad_norm': 1.2241822481155396, 'learning_rate': 5.0920245398773e-07, 'epoch': 0.49}
{'loss': 0.2809, 'grad_norm': 1.0673856735229492, 'learning_rate': 4.4785276073619634e-07, 'epoch': 0.55}
{'loss': 0.2423, 'grad_norm': 1.0089166164398193, 'learning_rate': 3.8650306748466255e-07, 'epoch': 0.61}
{'loss': 0.2197, 'grad_norm': 0.9601132273674011, 'learning_rate': 3.251533742331288e-07, 'epoch': 0.67}
{'loss': 0.2034, 'grad_norm': 0.8258605599403381, 'learning_rate': 2.6380368098159506e-07, 'epoch': 0.74}
{'loss': 0.1899, 'grad_norm': 1.0885920524597168, 'learning_rate': 2.0245398773006135e-07, 'epoch': 0.8}
{'loss': 0.1788, 'grad_norm': 0.7931262254714966, 'learning_rate': 1.4110429447852758e-07, 'epoch': 0.86}
{'loss': 0.1748, 'grad_norm': 0.9637917876243591, 'learning_rate': 7.975460122699386e-08, 'epoch': 0.92}
{'loss': 0.1726, 'grad_norm': 0.8108744025230408, 'learning_rate': 1.8404907975460124e-08, 'epoch': 0.98}
{'train_runtime': 1221.0415, 'train_samples_per_second': 818.973, 'train_steps_per_second': 0.4, 'train_loss': 0.37404646195760777, 'epoch': 1.0}
