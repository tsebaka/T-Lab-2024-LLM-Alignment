{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9111370,"sourceType":"datasetVersion","datasetId":5499302},{"sourceId":191289893,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:49:58.168981Z","iopub.execute_input":"2024-08-05T16:49:58.169399Z","iopub.status.idle":"2024-08-05T16:50:10.792382Z","shell.execute_reply.started":"2024-08-05T16:49:58.169367Z","shell.execute_reply":"2024-08-05T16:50:10.791123Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (0.9.6)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.42.3)\nRequirement already satisfied: numpy<2.0.0,>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.32.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.20.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl) (0.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.5.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport transformers\nimport datasets\n\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom trl import RewardTrainer, RewardConfig\nfrom torch.optim import AdamW\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:10.795054Z","iopub.execute_input":"2024-08-05T16:50:10.795476Z","iopub.status.idle":"2024-08-05T16:50:10.803181Z","shell.execute_reply.started":"2024-08-05T16:50:10.795437Z","shell.execute_reply":"2024-08-05T16:50:10.801999Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"markdown","source":"## kl","metadata":{}},{"cell_type":"code","source":"def compute_kl_loss(model1, model2, inputs):\n    output1 = model1(inputs).logits\n    output2 = model2(inputs).logits\n    \n    loss = torch.nn.functional.kl_div(\n        torch.nn.functional.log_softmax(output1, dim=-1),\n        torch.nn.functional.softmax(output2, dim=-1),\n        reduction='batchmean',\n    )\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:10.804605Z","iopub.execute_input":"2024-08-05T16:50:10.804939Z","iopub.status.idle":"2024-08-05T16:50:10.820952Z","shell.execute_reply.started":"2024-08-05T16:50:10.804910Z","shell.execute_reply":"2024-08-05T16:50:10.820031Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## slerp","metadata":{}},{"cell_type":"code","source":"def slerp(theta_init, theta1, theta2, lambd):\n    eps = 0.01\n    interpolated_weights = {}\n    for key in theta_init.keys():\n        delta1 = theta1[key] - theta_init[key]\n        delta2 = theta2[key] - theta_init[key]\n        omega = (np.dot(theta1[key].flatten().cpu(), theta2[key].flatten().cpu()) /\n                          (np.linalg.norm(theta1[key].flatten().cpu() + eps) * np.linalg.norm(theta2[key].flatten().cpu() + eps)))\n        omega = np.arccos(np.min((np.max((-0.99, omega)), 0.99)))\n        \n        interpolated_weights[key] = (theta_init[key] +\n                                     (np.sin((1 - lambd) * omega) / np.sin(omega)) * delta1 +\n                                     (np.sin(lambd * omega) / np.sin(omega)) * delta2)\n    return interpolated_weights","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:10.823676Z","iopub.execute_input":"2024-08-05T16:50:10.824042Z","iopub.status.idle":"2024-08-05T16:50:10.835082Z","shell.execute_reply.started":"2024-08-05T16:50:10.824011Z","shell.execute_reply":"2024-08-05T16:50:10.834284Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## utils","metadata":{}},{"cell_type":"code","source":"def evaluate(\n    reward_model,\n    reward_tokenizer,\n    sft_model, \n    sft_tokenizer,\n    prompts,\n\n):\n    init_model = GPT2LMHeadModel.from_pretrained(\n            SFT_PATH\n    ).to(device)\n    init_model.generation_config.pad_token_id = sft_tokenizer.pad_token_id\n    \n    reward_model.eval()\n    sft_model.eval()\n    \n    reward = []\n    avg_kl = []\n\n    for prompt in tqdm(prompts):\n        inputs = sft_tokenizer.encode_plus(\n            prompt,\n            return_tensors='pt', \n            truncation=True,\n            padding='max_length',\n            max_length=25\n        )\n        output = sft_model.generate(**inputs.to(device), max_length=40)\n        generation = sft_tokenizer.decode(output[0], skip_special_tokens=True)\n\n        inputs = reward_tokenizer.encode_plus(\n            generation,\n            return_tensors='pt',\n            truncation=True, \n            padding='max_length',\n            max_length=128\n        )\n        score = reward_model.forward(**inputs.to(device)).logits\n        \n        kl_loss = compute_kl_loss(\n                sft_model, \n                init_model,\n                inputs[\"input_ids\"],\n        )\n\n        reward.append(score.item())\n        avg_kl.append(kl_loss.item())\n    return np.mean(reward), np.mean(avg_kl)\n\ndef compute_reward(prompt):\n    inputs = reward_tokenizer.encode_plus(\n        prompt,\n        return_tensors='pt',\n        truncation=True, \n        max_length=64).to(device)\n    with torch.no_grad():\n        outputs = reward_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    logits = outputs.logits\n    reward = logits[0]\n    return reward\n\ndef generate_prompt(prompt):\n    inputs = sft_tokenizer.encode(prompt, return_tensors='pt').to(device)\n    outputs = sft_model.generate(inputs, max_length=50, num_return_sequences=1)\n    generated_prompt = sft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_prompt","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:10.836692Z","iopub.execute_input":"2024-08-05T16:50:10.837016Z","iopub.status.idle":"2024-08-05T16:50:10.850085Z","shell.execute_reply.started":"2024-08-05T16:50:10.836986Z","shell.execute_reply":"2024-08-05T16:50:10.848993Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"stanfordnlp/imdb\")\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\n\nprompts = []\n\nfor comment in tqdm(np.random.choice(test_dataset, 200)):\n    prompts.append(comment[\"text\"][:20])\n    \ntrain_prompts = []\nfor comment in tqdm(train_dataset):\n    train_prompts.append(comment[\"text\"][:20])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:10.851218Z","iopub.execute_input":"2024-08-05T16:50:10.851773Z","iopub.status.idle":"2024-08-05T16:50:16.743010Z","shell.execute_reply.started":"2024-08-05T16:50:10.851740Z","shell.execute_reply":"2024-08-05T16:50:16.741906Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:00<00:00, 211353.19it/s]\n100%|██████████| 25000/25000 [00:00<00:00, 25730.15it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# reward (pretrained)","metadata":{}},{"cell_type":"code","source":"PATH_TO_MODEL = \"/kaggle/input/reward-model\"\nREWARD_PATH = \"distilbert/distilbert-base-cased\"\n\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n    PATH_TO_MODEL,\n    num_labels=1\n).to(device)\n\nreward_tokenizer = AutoTokenizer.from_pretrained(REWARD_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:16.744400Z","iopub.execute_input":"2024-08-05T16:50:16.744787Z","iopub.status.idle":"2024-08-05T16:50:17.111340Z","shell.execute_reply.started":"2024-08-05T16:50:16.744755Z","shell.execute_reply":"2024-08-05T16:50:17.110280Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# sft","metadata":{}},{"cell_type":"code","source":"SFT_PATH = \"lvwerra/gpt2-imdb\"\n\nsft_model = GPT2LMHeadModel.from_pretrained(\n    SFT_PATH\n).to(device)\nsft_tokenizer = GPT2Tokenizer.from_pretrained(SFT_PATH)\n\nsft_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\nsft_tokenizer.pad_token = sft_tokenizer.eos_token\nsft_model.generation_config.pad_token_id = sft_tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:17.113060Z","iopub.execute_input":"2024-08-05T16:50:17.113473Z","iopub.status.idle":"2024-08-05T16:50:18.251419Z","shell.execute_reply.started":"2024-08-05T16:50:17.113438Z","shell.execute_reply":"2024-08-05T16:50:18.250588Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# reward & kl before rlhf stage","metadata":{}},{"cell_type":"code","source":"evaluate(\n    reward_model,\n    reward_tokenizer,\n    sft_model, \n    sft_tokenizer,\n    prompts\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:18.252616Z","iopub.execute_input":"2024-08-05T16:50:18.252931Z","iopub.status.idle":"2024-08-05T16:50:46.249957Z","shell.execute_reply.started":"2024-08-05T16:50:18.252905Z","shell.execute_reply":"2024-08-05T16:50:46.249012Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:27<00:00,  7.38it/s]\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(0.28810329470783475, -1.737438951820991e-07)"},"metadata":{}}]},{"cell_type":"markdown","source":"# RLHF","metadata":{}},{"cell_type":"code","source":"reward_model.eval()\nsft_model.train()\n\noptimizer = AdamW(sft_model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:46.253199Z","iopub.execute_input":"2024-08-05T16:50:46.253735Z","iopub.status.idle":"2024-08-05T16:50:46.262299Z","shell.execute_reply.started":"2024-08-05T16:50:46.253709Z","shell.execute_reply":"2024-08-05T16:50:46.261573Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"temp_model = GPT2LMHeadModel.from_pretrained(SFT_PATH).to(device)\n\nI = 2\nM = 2\nT = 100\nmu = 0.01\nlambd = 0.5\nnu = 0.5\nbeta = 0.1\ngamma = 0.99\nk = 10\n\ntheta_init = sft_model.state_dict()\ntemp_model = GPT2LMHeadModel.from_pretrained(SFT_PATH).to(device)\n\nfor i in range(I):\n    theta_m_list = []\n    theta_m_ema_list = []\n\n    for m in range(M):\n        theta_m = {name: v.clone() for name, v in theta_init.items()}\n        theta_m_ema = {name: v.clone() for name, v in theta_init.items()}\n        for prompt in tqdm(train_prompts[:T]):\n            generated_prompts = [generate_prompt(prompt) for _ in range(k)]\n\n            rewards = [compute_reward(generated_prompt) for generated_prompt in generated_prompts]\n\n            inputs = [sft_tokenizer.encode(generated_prompt, return_tensors='pt').to(device) for generated_prompt in generated_prompts]\n            outputs = [sft_model(input_tensor, labels=input_tensor) for input_tensor in inputs]\n            losses = [output.loss for output in outputs]\n            log_probs = [-loss for loss in losses]\n            \n            rloo_grads = []\n            for i in range(k):\n                other_rewards = [rewards[j] for j in range(k) if j != i]\n                baseline = sum(other_rewards) / (k - 1)\n                adjusted_reward = rewards[i] - baseline\n\n                log_prob = -log_probs[i]\n                rloo_grads.append(adjusted_reward * log_prob)\n                \n            kl_losses = torch.tensor([compute_kl_loss(sft_model, temp_model, input_tensor) for input_tensor in inputs])\n\n            rloo_grad = sum(rloo_grads) / k - beta * kl_losses.mean() * torch.tensor(log_prob).mean()\n            rloo_grad.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            for param_name, param_value in theta_m.items():\n                theta_m_ema[param_name] = (1 - mu) * theta_m_ema[param_name] + mu * param_value\n\n            temp_model.load_state_dict(theta_m_ema)\n\n        theta_m_list.append(theta_m)\n        theta_m_ema_list.append(theta_m_ema)\n\n    theta_slerp = slerp(theta_init, theta_m_list[0], theta_m_list[1], lambd)\n\n    for name in theta_init.keys():\n        theta_init[name] = (1 - nu) * theta_init[name] + nu * theta_slerp[name]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:50:46.263642Z","iopub.execute_input":"2024-08-05T16:50:46.263910Z","iopub.status.idle":"2024-08-05T17:20:46.930435Z","shell.execute_reply.started":"2024-08-05T16:50:46.263888Z","shell.execute_reply":"2024-08-05T17:20:46.929588Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_34/767301243.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  rloo_grad = sum(rloo_grads) / k - beta * kl_losses.mean() * torch.tensor(log_prob).mean()\n100%|██████████| 100/100 [07:14<00:00,  4.35s/it]\n100%|██████████| 100/100 [07:28<00:00,  4.49s/it]\n100%|██████████| 100/100 [07:35<00:00,  4.55s/it]\n100%|██████████| 100/100 [07:36<00:00,  4.56s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# KL-reward Pareto front of weight","metadata":{}},{"cell_type":"code","source":"def interpolate_weights(theta_sft, theta_slerp, eta):\n    interpolated_weights = {}\n    for param_name in theta_sft.keys():\n        interpolated_weights[param_name] = (1 - eta) * theta_sft[param_name] + eta * theta_slerp[param_name]\n    return interpolated_weights\n\nweights = interpolate_weights(sft_model.state_dict(), theta_slerp, 1 / 2)\n\nsft_model.load_state_dict(weights)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T17:23:14.775905Z","iopub.execute_input":"2024-08-05T17:23:14.776351Z","iopub.status.idle":"2024-08-05T17:23:14.801863Z","shell.execute_reply.started":"2024-08-05T17:23:14.776322Z","shell.execute_reply":"2024-08-05T17:23:14.800875Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# reward & kl after rlhf stage","metadata":{}},{"cell_type":"code","source":"# 100 with RLOO\n\nevaluate(\n    reward_model,\n    reward_tokenizer,\n    sft_model, \n    sft_tokenizer,\n    prompts\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T17:23:28.466252Z","iopub.execute_input":"2024-08-05T17:23:28.466995Z","iopub.status.idle":"2024-08-05T17:23:57.271757Z","shell.execute_reply.started":"2024-08-05T17:23:28.466960Z","shell.execute_reply":"2024-08-05T17:23:57.270352Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [00:27<00:00,  7.19it/s]\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(2.419940478205681, 6.123236216306687)"},"metadata":{}}]},{"cell_type":"code","source":"for i, prompt in enumerate(prompts):\n    print(sft_tokenizer.decode(\n        sft_model.generate(\n            sft_tokenizer.encode(\n            prompt, \n            return_tensors='pt').to(device))[0])\n    )\n    if i == 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-05T17:23:57.273757Z","iopub.execute_input":"2024-08-05T17:23:57.274589Z","iopub.status.idle":"2024-08-05T17:23:58.698722Z","shell.execute_reply.started":"2024-08-05T17:23:57.274547Z","shell.execute_reply":"2024-08-05T17:23:58.697895Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Well, to each his owlishness and his wit, and to the great and the small,\nThis third installme of the film is a great film. It is a great film. It is\nAfter being off the iced tea, I was very impressed with the film. I am very glad\nThis movie is my familar favorite movie of all time. I am very glad I did. I\nEarth Final Conflict, and the film is a great film. I am very glad I did. I\nBeing a long-time favela fan, I was very impressed with the film. I am\nA gentle story, hinting at the future of the film, and a great cast. The film\nOn his recent malignity, the film is a great film. It is a great film.\nThe movie, which was released in the UK in the early 1980s, is a great film.\nI love sci-fi and am a fan of the original series. I am very glad I did\nOK, this movie, was iced in by a great cast and a great story. I am\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# 500 with RLOO\n# beta = 0.01\nevaluate(\n    reward_model,\n    reward_tokenizer,\n    sft_model, \n    sft_tokenizer,\n    prompts\n)","metadata":{},"execution_count":15,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 200/200 [00:20<00:00,  9.66it/s]\n"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":["(1.9629860120639204, 23.38014699459076)"]},"metadata":{}}]},{"cell_type":"code","source":"for i, prompt in enumerate(prompts):\n    print(sft_tokenizer.decode(\n        sft_model.generate(\n            sft_tokenizer.encode(\n            prompt, \n            return_tensors='pt').to(device))[0])\n    )\n    if i == 10:\n        break","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Some time ago I saw  the film in a theater and I was very impressed with the film\n\nRobert Montgomery an excellent actor, and the film is a great film. It is a great film.\n\nI've just revisited  the film and I think it is a great film. I think\n\nI'll be brief: I nor anyone else in the film has ever seen a film that is as\n\nNational Lampoon's Cagney, and the film is a great film. It is a great\n\nLast fall (of 2001), the film is a great film. It is a great film. It\n\nI have done some resounding reviews of this film, and I think it is a great film.\n\nThis movie was a comical film, and it is a great film. It is a great film\n\nBack in college I strolled through the film with a group of friends and we were all impressed with\n\nThe Evil is about a ersatz, middle-aged man who is forced to live with his\n\nI bought this movie ive seen it a few times and i think it is a great film.\n"}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# 500 with RLOO\n# beta = 0.1\n\nevaluate(\n    reward_model,\n    reward_tokenizer,\n    sft_model, \n    sft_tokenizer,\n    prompts\n)","metadata":{},"execution_count":12,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 200/200 [00:18<00:00, 10.56it/s]\n"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":["(1.8983262504637242, 124.56155834197997)"]},"metadata":{}}]},{"cell_type":"code","source":"for i, prompt in enumerate(prompts):\n    print(sft_tokenizer.decode(\n        sft_model.generate(\n            sft_tokenizer.encode(\n            prompt, \n            return_tensors='pt').to(device))[0])\n    )\n    if i == 10:\n        break","metadata":{},"execution_count":13,"outputs":[{"name":"stderr","output_type":"stream","text":"/home/jovyan/.local/share/virtualenvs/ptls-experiments-w-dEu3oS/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n\n  warnings.warn(\n"},{"name":"stdout","output_type":"stream","text":"Egads.<br /><br />I ive seen a lot of great films, and\n\nThis is one of the fifties' best films), and it is a very good film.\n\nThis was it! i would say it was a very good film). The film is a very good\n\nbefore seeing this fable, I was very impressed with the film). The film is a very good\n\nAmitabh and Jiah Khaalid), a young man who is in love with a\n\nOften when TV series are being made, they are often very good), and the film is a very\n\nIf this is the best ive seen of the film, it is a very good film), and\n\nMANNA FROM HEAVEN is a very good film). It is a very good film. It\n\n\"Beyond Rangoon\" is ersatz, and it is a very good film. It\n\nA horrible mish mash of the original and the film's sequel, this film is a great film.\n\nThis movie was a disjointed mess, and I was very impressed with the film. It is\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}