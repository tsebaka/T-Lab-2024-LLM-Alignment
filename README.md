# T-Lab-2024-LLM-Alignment

## Mini-batch итогов
Сначала я реализовал метод reinforce (sorry trl в вашем коде я нереально запутался) и потом добавил метод WARP (а именно EMA, Slerp, LITI).
После этого я заметил, что то, что генерит sft-шка это почти всегда GOOD MOVIE GOOD MOVIE (без сильного подбора beta и T), и поэтому я подумал что нужно добавить метод RLOO чтобы дисперсия была меньше и модель лучше обучалась на k разных развитий генерации (взял отсюда [Back to Basics](https://arxiv.org/pdf/2402.14740)). Что удивительно (типа реально заработало) у меня результаты reward'а увеличилось и ответы были более разнообразные а не просто FUCK ITS A GREAT MOVIE.

## Ultra-mini-batch того что я понял
Результаты были очень понятны (увеличиваешь бету, модель хуже генерит хорошие комменты, перетягивая всё на kl loss, соответственно avg_reward_before &asymp; avg_reward_after и avg_kl_before &asymp; avg_kl_after)
Кол-во параметров T

## Таблица

| Заголовок 1 | Заголовок 2 | Заголовок 3 |
|-------------|-------------|-------------|
| Значение 1  | Значение 2  | Значение 3  |
| Значение 4  | Значение 5  | Значение 6  |
| Значение 7  | Значение 8  | Значение 9  |

## Установка

Инструкции по установке.

## Использование

Примеры использования.





Hardware
Nvidia A100 80gb
